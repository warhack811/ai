"""
services/llm/model_router.py - OPTIMIZE EDÄ°LMÄ°Åž VERSÄ°YON
-----------------------------------------------------------
âœ… HÄ±z + Kalite dengesi kuruldu
âœ… QWEN sadece gerektiÄŸinde kullanÄ±lÄ±yor
âœ… DeepSeek modeli dÃ¼zeltildi
"""

from __future__ import annotations

import logging
from typing import Optional, Tuple

from schemas.common import ChatMode, IntentLabel
from schemas.chat import ChatRequest
from .complexity_scorer import ComplexityScorer
from .model_manager import (
    LLMModelInfo,
    generate_with_model,
    get_model_info,
    get_primary_model,
    list_all_models,
)

logger = logging.getLogger(__name__)

# Global complexity scorer
_complexity_scorer = ComplexityScorer()


# ---------------------------------------------------------------------------
# OPTÄ°MÄ°ZE EDÄ°LMÄ°Åž MODEL SEÃ‡Ä°MÄ° (HÄ±z + Kalite Dengesi)
# ---------------------------------------------------------------------------

def select_model_by_complexity(
    query: str,
    mode: ChatMode,
    intent: Optional[IntentLabel] = None,
    force_model: Optional[str] = None,
) -> Tuple[str, int]:
    """
    ðŸŽ¯ AKILLI MODEL SEÃ‡Ä°MÄ°
    
    Strateji:
    - Basit sorular â†’ Phi (hÄ±zlÄ±)
    - Kod sorularÄ± â†’ Mistral (hÄ±zlÄ± + yetenekli)
    - KarÅŸÄ±laÅŸtÄ±rma/Ã–neri â†’ Mistral (QWEN yerine, hÄ±z iÃ§in)
    - AÃ§Ä±klama (uzun) â†’ Mistral
    - Ã‡ok karmaÅŸÄ±k â†’ QWEN (kalite lazÄ±msa)
    - Reasoning â†’ DeepSeek (nadiren)
    
    Returns:
        (model_key, complexity_score)
    """
    # Force model varsa direkt kullan
    if force_model:
        info = get_model_info(force_model)
        if info:
            return force_model, 5
        logger.warning(f"Force model {force_model} bulunamadÄ±, otomatik seÃ§im yapÄ±lÄ±yor")
    
    # Complexity skorla
    complexity = _complexity_scorer.score(query, mode, intent)
    query_lower = query.lower()
    
    logger.debug(f"Query complexity: {complexity}/10 | Intent: {intent}")
    
    # Mevcut modelleri kontrol et
    all_models = list_all_models()
    
    # ============================================================
    # Ã–NCELÄ°KLÄ° DURUMLAR (Intent-based override)
    # ============================================================
    
    # 1. Small talk â†’ Phi (en hÄ±zlÄ±)
    if intent == IntentLabel.SMALL_TALK:
        return "phi", 1
    
    # 2. Kod sorularÄ± â†’ Mistral (hÄ±zlÄ± + kod konusunda iyi)
    if (intent == IntentLabel.CODE_HELP or 
        mode == ChatMode.CODE or
        any(kw in query_lower for kw in ['kod', 'code', 'python', 'javascript', 'fonksiyon', 'class'])):
        return "mistral", complexity
    
    # 3. Ã‡eviri â†’ Phi (basit)
    if intent == IntentLabel.TRANSLATE:
        return "phi", 2
    
    # 4. Ã–zet â†’ Mistral (orta seviye)
    if intent == IntentLabel.SUMMARIZE:
        return "mistral", complexity
    
    # ============================================================
    # COMPLEXITY-BASED SEÃ‡Ä°M
    # ============================================================
    
    # Complexity 0-3: Basit sorular â†’ Phi
    if complexity <= 3:
        return "phi", complexity
    
    # Complexity 4-6: Orta sorular â†’ Mistral (QWEN yerine!)
    elif complexity <= 6:
        # Ã–neri/karÅŸÄ±laÅŸtÄ±rma varsa Mistral yeterli
        if intent in [IntentLabel.EXPLAIN, IntentLabel.QUESTION]:
            return "mistral", complexity
        return "mistral", complexity
    
    # Complexity 7-8: KarmaÅŸÄ±k â†’ QWEN (ama dikkatli!)
    elif complexity <= 8:
        # Sadece gerÃ§ekten gerektiÄŸinde QWEN kullan
        if intent in [IntentLabel.EMOTIONAL_SUPPORT, IntentLabel.WEB_SEARCH]:
            return "qwen", complexity
        
        # Uzun aÃ§Ä±klama istiyorsa QWEN
        if len(query.split()) > 20:
            return "qwen", complexity
        
        # DiÄŸer durumlarda Mistral yeterli
        return "mistral", complexity
    
    # Complexity 9-10: Ã‡ok karmaÅŸÄ±k â†’ DeepSeek (reasoning)
    else:
        if "deepseek" in all_models:
            return "deepseek", complexity
        # DeepSeek yoksa QWEN fallback
        return "qwen", complexity


# ---------------------------------------------------------------------------
# Routed Generate with Complexity
# ---------------------------------------------------------------------------

async def route_and_generate(
    chat_request: ChatRequest,
    composed_prompt: str,
    system_prompt: str,
    *,
    force_model: Optional[str] = None,
    override_temperature: Optional[float] = None,
    override_max_tokens: Optional[int] = None,
    intent: Optional[IntentLabel] = None,
    user_message: str = "",
    context: str = "",
    mode: Optional[ChatMode] = None,
) -> Tuple[str, str]:
    """
    FAS 1: Complexity-based routing with native templates
    
    Returns:
        (answer_text, used_model_key)
    """
    # Mode'u belirle
    if mode is None:
        mode = chat_request.mode
    
    # user_message yoksa request'ten al
    if not user_message:
        user_message = chat_request.message
    
    # context yoksa composed_prompt kullan
    if not context:
        context = composed_prompt
    
    msg = chat_request.message

    # Complexity-based selection
    model_key, complexity = select_model_by_complexity(
        query=msg,
        mode=mode,
        intent=intent,
        force_model=force_model,
    )
    
    info: Optional[LLMModelInfo] = get_model_info(model_key)

    # Temperature / max_tokens
    temp = override_temperature
    max_toks = override_max_tokens

    if info:
        if temp is None:
            temp = info.default_temperature
        if max_toks is None:
            max_toks = info.default_max_tokens

    logger.info(
        "ðŸŽ¯ Routed to: %s (complexity=%d/10) | temp=%.2f, max_tokens=%s",
        model_key.upper(),
        complexity,
        temp if temp is not None else -1,
        max_toks,
    )

    # Generate with native templates
    text = await generate_with_model(
        model_key=model_key,
        prompt=composed_prompt,
        system_prompt=system_prompt,
        temperature=temp,
        max_tokens=max_toks,
        user_message=user_message,
        context=context,
        mode=mode,
    )

    return text, model_key